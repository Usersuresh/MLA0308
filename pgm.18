import numpy as np
import random

class TicTacToe:
    def __init__(self):
        self.board = [' '] * 9  # Initialize the board
        self.current_player = 'X'  # Player X starts the game
        self.winning_combinations = [
            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Rows
            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Columns
            [0, 4, 8], [2, 4, 6]              # Diagonals
        ]
        self.state_space_size = 3 ** 9  # Number of possible board configurations
        self.action_space_size = 9  # Number of possible actions (positions to place 'X' or 'O')
        self.q_table = np.zeros((self.state_space_size, self.action_space_size))  # Q-table initialization
        self.learning_rate = 0.1
        self.discount_factor = 0.9
        self.epsilon = 0.1

    def reset(self):
        self.board = [' '] * 9
        self.current_player = 'X'

    def get_state(self):
        return ''.join(self.board)  # Concatenate board positions to represent the state

    def get_valid_moves(self):
        return [i for i, v in enumerate(self.board) if v == ' ']

    def is_winner(self, player):
        return any(all(self.board[i] == player for i in combo) for combo in self.winning_combinations)

    def is_draw(self):
        return ' ' not in self.board

    def is_terminal(self):
        return self.is_winner('X') or self.is_winner('O') or self.is_draw()

    def step(self, action):
        self.board[action] = self.current_player
        reward = 0
        if self.is_winner(self.current_player):
            reward = 1
        elif self.is_draw():
            reward = 0
        self.current_player = 'O' if self.current_player == 'X' else 'X'
        return self.get_state(), reward, self.is_terminal()

    def update_q_table(self, state, action, reward, next_state):
        current_q_value = self.q_table[state][action]
        max_next_q_value = np.max(self.q_table[next_state])
        new_q_value = (1 - self.learning_rate) * current_q_value + \
                      self.learning_rate * (reward + self.discount_factor * max_next_q_value)
        self.q_table[state][action] = new_q_value

    def choose_action(self, state):
        if random.random() < self.epsilon:
            return random.choice(self.get_valid_moves())
        else:
            return np.argmax(self.q_table[state])

# Train the Q-learning agent
def train_q_learning_agent(env, episodes=1000):
    for episode in range(episodes):
        state = env.reset()
        while True:
            action = env.choose_action(state)
            next_state, reward, done = env.step(action)
            env.update_q_table(state, action, reward, next_state)
            state = next_state
            if done:
                break

# Test the trained Q-learning agent
def test_q_learning_agent(env, episodes=100):
    wins, draws, losses = 0, 0, 0
    for _ in range(episodes):
        state = env.reset()
        while True:
            action = np.argmax(env.q_table[state])
            next_state, reward, done = env.step(action)
            if done:
                if reward == 1:
                    wins += 1
                elif reward == 0:
                    draws += 1
                break
            state = next_state
    print(f"Wins: {wins}, Draws: {draws}, Losses: {episodes - wins - draws}")

if __name__ == "__main__":
    env = TicTacToe()
    train_q_learning_agent(env)
    test_q_learning_agent(env)
