import numpy as np

class GridWorld:
    def __init__(self, size=5):
        self.size = size
        self.grid = np.zeros((size, size))
        self.start = (0, 0)
        self.goal = (size-1, size-1)
        self.obstacles = [(1, 1), (2, 2), (3, 3)]
        self.agent_position = self.start
        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']
    
    def reset(self):
        self.agent_position = self.start
        return self.agent_position
    
    def step(self, action):
        if action == 'UP' and self.agent_position[0] > 0:
            self.agent_position = (self.agent_position[0] - 1, self.agent_position[1])
        elif action == 'DOWN' and self.agent_position[0] < self.size - 1:
            self.agent_position = (self.agent_position[0] + 1, self.agent_position[1])
        elif action == 'LEFT' and self.agent_position[1] > 0:
            self.agent_position = (self.agent_position[0], self.agent_position[1] - 1)
        elif action == 'RIGHT' and self.agent_position[1] < self.size - 1:
            self.agent_position = (self.agent_position[0], self.agent_position[1] + 1)
        
        reward = 0
        if self.agent_position == self.goal:
            reward = 1
        elif self.agent_position in self.obstacles:
            reward = -0.5
        
        return self.agent_position, reward
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

class PPOAgent:
    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.95, epsilon=0.2):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.policy_model = self._build_model()
        self.optimizer = Adam(lr=learning_rate)
    
    def _build_model(self):
        model = Sequential([
            Dense(32, activation='relu', input_shape=(self.state_size,)),
            Dense(16, activation='relu'),
            Dense(self.action_size, activation='softmax')
        ])
        return model
    
    def choose_action(self, state):
        state = np.reshape(state, [1, self.state_size])
        probabilities = self.policy_model.predict(state)[0]
        action = np.random.choice(self.action_size, p=probabilities)
        return action
    
    def update_policy(self, states, actions, advantages):
        states = np.array(states)
        actions = np.array(actions)
        advantages = np.array(advantages)
        
        with tf.GradientTape() as tape:
            logits = self.policy_model(states, training=True)
            action_masks = tf.one_hot(actions, self.action_size)
            selected_logits = tf.reduce_sum(action_masks * logits, axis=1)
            ratios = tf.exp(selected_logits - logits)
            surr1 = ratios * advantages
            surr2 = tf.clip_by_value(ratios, 1 - self.epsilon, 1 + self.epsilon) * advantages
            loss = -tf.reduce_mean(tf.minimum(surr1, surr2))
        
        gradients = tape.gradient(loss, self.policy_model.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.policy_model.trainable_variables))
